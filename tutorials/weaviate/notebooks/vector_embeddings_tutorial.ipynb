{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector database tutorial\n",
    "\n",
    "Using [Weaviate](https://weaviate.io/developers/weaviate/installation/embedded) as open-source vector database.\n",
    "\n",
    "### Setup vector database\n",
    "\n",
    "Download some sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 10\n",
      "{\n",
      "  \"Category\": \"SCIENCE\",\n",
      "  \"Question\": \"This organ removes excess glucose from the blood & stores it as glycogen\",\n",
      "  \"Answer\": \"Liver\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Download the data\n",
    "resp = requests.get('https://raw.githubusercontent.com/weaviate-tutorials/quickstart/main/data/jeopardy_tiny.json')\n",
    "data = json.loads(resp.text)  # Load data\n",
    "\n",
    "# Parse the JSON and preview it\n",
    "print(type(data), len(data))\n",
    "\n",
    "def json_print(data):\n",
    "    print(json.dumps(data, indent=2))\n",
    "\n",
    "json_print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an embedded instance of Weaviate vector database\n",
    "\n",
    "Provide API key for OpenAI in order to use their models for vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/longbe01/Documents/projects/llm-rag/venv-llm-rag/lib/python3.10/site-packages/weaviate/__init__.py:130: DeprecationWarning: Dep010: Importing EmbeddedOptions from weaviate is deprecated. Import EmbeddedOptions from its module: weaviate.embedded\n",
      "  _Warnings.root_module_import(name, map_[name])\n"
     ]
    }
   ],
   "source": [
    "import weaviate, os\n",
    "from weaviate import EmbeddedOptions\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/longbe01/Documents/projects/llm-rag/venv-llm-rag/lib/python3.10/site-packages/weaviate/warnings.py:162: DeprecationWarning: Dep016: Python client v3 `weaviate.Client(...)` connections and methods are deprecated. Update\n",
      "            your code to use Python client v4 `weaviate.WeaviateClient` connections and methods.\n",
      "\n",
      "            For Python Client v4 usage, see: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "            For code migration, see: https://weaviate.io/developers/weaviate/client-libraries/python/v3_v4_migration\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary /Users/longbe01/.cache/weaviate-embedded did not exist. Downloading binary from https://github.com/weaviate/weaviate/releases/download/v1.23.7/weaviate-v1.23.7-Darwin-all.zip\n",
      "Started /Users/longbe01/.cache/weaviate-embedded: process ID 92469\n",
      "Client created? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"startup\",\"default_vectorizer_module\":\"none\",\"level\":\"info\",\"msg\":\"the default vectorizer modules is set to \\\"none\\\", as a result all new schema classes without an explicit vectorizer setting, will use this vectorizer\",\"time\":\"2024-05-21T09:08:00+01:00\"}\n",
      "{\"action\":\"startup\",\"auto_schema_enabled\":true,\"level\":\"info\",\"msg\":\"auto schema enabled setting is set to \\\"true\\\"\",\"time\":\"2024-05-21T09:08:00+01:00\"}\n",
      "{\"level\":\"info\",\"msg\":\"No resource limits set, weaviate will use all available memory and CPU. To limit resources, set LIMIT_RESOURCES=true\",\"time\":\"2024-05-21T09:08:00+01:00\"}\n",
      "{\"level\":\"warning\",\"msg\":\"Multiple vector spaces are present, GraphQL Explore and REST API list objects endpoint module include params has been disabled as a result.\",\"time\":\"2024-05-21T09:08:00+01:00\"}\n",
      "{\"action\":\"grpc_startup\",\"level\":\"info\",\"msg\":\"grpc server listening at [::]:50060\",\"time\":\"2024-05-21T09:08:00+01:00\"}\n",
      "{\"action\":\"restapi_management\",\"level\":\"info\",\"msg\":\"Serving weaviate at http://127.0.0.1:8079\",\"time\":\"2024-05-21T09:08:00+01:00\"}\n"
     ]
    }
   ],
   "source": [
    "client = weaviate.Client(\n",
    "    embedded_options=EmbeddedOptions(),\n",
    "    additional_headers={\n",
    "        \"X-OpenAI-BaseURL\": os.environ['OPENAI_API_BASE'],\n",
    "        \"X-OpenAI-Api-Key\": openai.api_key  # Replace this with your actual key\n",
    "    }\n",
    ")\n",
    "print(f\"Client created? {client.is_ready()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"hostname\": \"http://127.0.0.1:8079\",\n",
      "  \"modules\": {\n",
      "    \"generative-openai\": {\n",
      "      \"documentationHref\": \"https://platform.openai.com/docs/api-reference/completions\",\n",
      "      \"name\": \"Generative Search - OpenAI\"\n",
      "    },\n",
      "    \"qna-openai\": {\n",
      "      \"documentationHref\": \"https://platform.openai.com/docs/api-reference/completions\",\n",
      "      \"name\": \"OpenAI Question & Answering Module\"\n",
      "    },\n",
      "    \"ref2vec-centroid\": {},\n",
      "    \"reranker-cohere\": {\n",
      "      \"documentationHref\": \"https://txt.cohere.com/rerank/\",\n",
      "      \"name\": \"Reranker - Cohere\"\n",
      "    },\n",
      "    \"text2vec-cohere\": {\n",
      "      \"documentationHref\": \"https://docs.cohere.ai/embedding-wiki/\",\n",
      "      \"name\": \"Cohere Module\"\n",
      "    },\n",
      "    \"text2vec-huggingface\": {\n",
      "      \"documentationHref\": \"https://huggingface.co/docs/api-inference/detailed_parameters#feature-extraction-task\",\n",
      "      \"name\": \"Hugging Face Module\"\n",
      "    },\n",
      "    \"text2vec-openai\": {\n",
      "      \"documentationHref\": \"https://platform.openai.com/docs/guides/embeddings/what-are-embeddings\",\n",
      "      \"name\": \"OpenAI Module\"\n",
      "    }\n",
      "  },\n",
      "  \"version\": \"1.23.7\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "json_print(client.get_meta())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create collection of questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"level\":\"info\",\"msg\":\"Created shard question_DQfoxUs1CT75 in 2.101917ms\",\"time\":\"2024-05-21T09:13:54+01:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":1000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-05-21T09:13:54+01:00\",\"took\":31250}\n"
     ]
    }
   ],
   "source": [
    "# resetting the schema. CAUTION: This will delete your collection \n",
    "if client.schema.exists(\"Question\"):\n",
    "    client.schema.delete_class(\"Question\")\n",
    "\n",
    "# \n",
    "class_obj = {\n",
    "    \"class\": \"Question\",\n",
    "    \"vectorizer\": \"text2vec-openai\",  # Use OpenAI as the vectorizer\n",
    "    \"moduleConfig\": {\n",
    "        \"text2vec-openai\": {\n",
    "            \"model\": \"ada\",\n",
    "            \"modelVersion\": \"002\",\n",
    "            \"type\": \"text\",\n",
    "            \"baseURL\": os.environ[\"OPENAI_API_BASE\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "client.schema.create_class(class_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in sample data and generate vector embeddings\n",
    "\n",
    "For our sample data, load this in to the vector db and use the settings above to generate vector embeddings. This calls the OpenAI API to convert the Questions/Answers/Categories into embeddings using one of the OpenAI text2vec models (ada in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reminder for the data structure\n",
    "json_print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with client.batch.configure(batch_size=5) as batch:\n",
    "    for i, d in enumerate(data):  # Batch import data\n",
    "        \n",
    "        print(f\"importing question: {i+1}\")\n",
    "        \n",
    "        properties = {\n",
    "            \"answer\": d[\"Answer\"],\n",
    "            \"question\": d[\"Question\"],\n",
    "            \"category\": d[\"Category\"],\n",
    "        }\n",
    "        \n",
    "        batch.add_data_object(\n",
    "            data_object=properties,\n",
    "            class_name=\"Question\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = client.query.aggregate(\"Question\").with_meta_count().do()\n",
    "json_print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the vector that represents each question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a query to extract the vector for a question\n",
    "result = (client.query\n",
    "          .get(\"Question\", [\"category\", \"question\", \"answer\"])\n",
    "          .with_additional(\"vector\")\n",
    "          .with_limit(1)\n",
    "          .do())\n",
    "\n",
    "json_print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = (\n",
    "    client.query\n",
    "    .get(\"Question\",[\"question\",\"answer\",\"category\"])\n",
    "    .with_near_text({\"concepts\": \"biology\"})\n",
    "    .with_additional('distance')\n",
    "    .with_limit(2)\n",
    "    .do()\n",
    ")\n",
    "\n",
    "json_print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = (\n",
    "    client.query\n",
    "    .get(\"Question\", [\"question\", \"answer\"])\n",
    "    .with_near_text({\"concepts\": [\"animals\"]})\n",
    "    .with_limit(10)\n",
    "    .with_additional([\"distance\"])\n",
    "    .do()\n",
    ")\n",
    "\n",
    "json_print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply a threshold distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = (\n",
    "    client.query\n",
    "    .get(\"Question\", [\"question\", \"answer\"])\n",
    "    .with_near_text({\"concepts\": [\"animals\"], \"distance\": 0.24})\n",
    "    .with_limit(10)\n",
    "    .with_additional([\"distance\"])\n",
    "    .do()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRUD operations in Vector DBs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an object\n",
    "object_uuid = client.data_object.create(\n",
    "    data_object={\n",
    "        'question':\"Leonardo da Vinci was born in this country.\",\n",
    "        'answer': \"Italy\",\n",
    "        'category': \"Culture\"\n",
    "    },\n",
    "    class_name=\"Question\"\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(object_uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read an object\n",
    "data_object = client.data_object.get_by_id(object_uuid, class_name=\"Question\")\n",
    "json_print(data_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_object = client.data_object.get_by_id(\n",
    "    object_uuid,\n",
    "    class_name='Question',\n",
    "    with_vector=True\n",
    ")\n",
    "\n",
    "json_print(data_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update an object\n",
    "client.data_object.update(\n",
    "    uuid=object_uuid,\n",
    "    class_name=\"Question\",\n",
    "    data_object={\n",
    "        'answer':\"Florence, Italy\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_object = client.data_object.get_by_id(\n",
    "    object_uuid,\n",
    "    class_name='Question',\n",
    ")\n",
    "json_print(data_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete an object\n",
    "json_print(client.query.aggregate(\"Question\").with_meta_count().do())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.data_object.delete(uuid=object_uuid, class_name=\"Question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_print(client.query.aggregate(\"Question\").with_meta_count().do())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
